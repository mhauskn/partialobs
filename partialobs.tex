\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsthm}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{assumption}{Assumption}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09
\title{}
\author{}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\nipsfinalcopy % Uncomment for camera-ready version
\begin{document}

\maketitle

\begin{abstract}
We seek to contribute:\\
1) A formal defintion of Partial Observability\\
2) A formal measure of the degree to which a domain is partially observable\\
3) A way to quantify the deficiency of a given state representation\\
\end{abstract}

\section{Definitions}

\begin{definition}[Markov Property]
A stochastic process has the Markov property if the conditional
probability distribution of future states of the process (conditional
on both past and present states) depends only upon the present state,
not on the sequence of events that preceded it.
\[
P(x_{n+1} | x_{n}, x_{n-1}, \dots, x_{1}) = P(x_{n+1} | x_{n})
\]
\end{definition}

\begin{definition}[k-order Markov Chain]
A Markov chain of order $k$ has the property that:
\[
P(x_{n+1} | x_{n}, x_{n-1}, \dots, x_{1}) = P(x_{n+1} | x_{n}, \dots, x_{n-(k-1)})
\]
The future state $x_{n+1}$ depends on the past $k$ states. Remembering
$k$ states restores the Markov property and a larger memory grants no
further predictive power.
\end{definition}

\begin{definition}[Entropy]
\label{def:entropy}
The entropy of a discrete random variable $X$ with alphabet $\mathcal{X}$ is
defined by:
\[
H(X) = \sum_{x\in \mathcal{X}} p(x) \log p(x)
\]
\end{definition}

\begin{definition}[Mutual Information]
The mutual information between two random variables $I(X;Y)$ is the
reduction of uncertainty of $X$ due to knowledge of $Y$:
\[
I(X;Y) = \sum_{x,y} p(x,y) \log \frac{p(x|y)}{p(x)} = H(X) - H(X|Y)
\]
\end{definition}

\section{Partial Observability}
In the context of reinforcement learning, partial observability
describes the category of agent-environment interactions in which an
agent's perceptions do not provide complete information about the true
state of the environment. This problem is also known as
\textit{incomplete perception}, \textit{perceptual aliasing}, or
simply \textit{hidden state}.

One cause of partial observability is sensor noise, in which two
different states appear to be the same state due to noise in
observations. Another cause is perceptual aliasing, in which multiple
environment states are indistinguishable given the
observations. Figure \ref{fig:functions} illustrates these cases.

\begin{figure}[htp]
\centering
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{figures/bijection}
  \caption{Fully Observed}
  \label{fig:bijection}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{figures/state-aliasing}
  \caption{State Aliasing}
  \label{fig:state-aliasing}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
  \centering
  \includegraphics[width=.6\linewidth]{figures/noisy-perception}
  \caption{Noisy Perceptions}
  \label{fig:noisy-perception}
\end{subfigure}
\caption{A fully observed environment features an injective mapping
  from true states to observations. Partial observability indicates
  the degree to which this injection is invalid. Two sources of
  partial observability are state aliasing, the degree to which the
  observation function is non-injective, and noisy perceptions, the
  degree to which the observation function is not a valid function.}
\label{fig:functions}
\end{figure}

\subsection{Criteria for Partial Observability}
\begin{assumption}
The underlying behavior of the environment operates according to a
Markov Decision Process. In particular, the environment features a
true state for which the Markov property holds.
\end{assumption}

The following conditions are necessary and sufficient for an
environment to be considered partially observable:

\begin{enumerate}
\item The environment provides observations rather than true states.
\item The mapping from true states to observations not injective (one-to-one).
\end{enumerate}

The non-injectivity in the mapping between true states and
observations determines the degree of partial observability.

\section{An Information-Theoretic View of Partial Observability}
One way to understand the partial observability of a domain is to
quantify the amount of information the observations provide about the
true states. Let us assume we have access to the true environment
state which generates each observation. Let $S$ be a random variable
over the domain of all possible states, and $O$ be a random variable
over all possible observations. The mutual information $I(S;O)$
computes the reduction in uncertainty of the true environment states
$S$ resulting from knowledge of the observations $O$:

\[
I(S;O) = \sum_{s,o \in S \times O} p(s,o) \log \frac{p(s|o)}{p(s)} = H(S) - H(S|O)
\]

If the mapping from states to observations is one-to-one, the mutual
information $I(S;O)$ will equal $H(S)$, and the agent's interaction
with the environment is said to be fully observed.

\section{Using a History of Observations}
A natural technique to deal with partial observability is to augment
the agent's state representation to include a history of
observations. This strategy helps to address state aliasing by using
the observation history to disambiguate aliased states. In the case of
noisy observations, a history could be used to counteract noise, for
example by computing an average. However, the size of the state space
grows exponentially as a function of history length, so in practice it
is rarely feasible to maintain extensive history.

\begin{definition}[k-th Order Mutual Information]
Let $\textbf{O}^k$ be a random variable over the domain of all
k-length observation sequences $\textbf{o}^k = (o_t, o_{t-1}, \dots,
o_{t-(k-1)})$. We define the $k$-th order mutual information between
$k$-length observation histories and true states as follows:

\[
I(S;\textbf{O}^k) = \sum_{s,\textbf{o}^k \in S \times \textbf{O}^k} p(s_t,\textbf{o}^k) \log \frac{p(s|\textbf{o}^k)}{p(s)}
\]
\end{definition}

The growth of $I(S;\textbf{O}^k)$ as a function of $k$ reveals the
predictive power gained by leveraging additional history. Even
maintaining a full history cannot guarantee full observability in all
cases: in an extreme case where all states are mapped to the state
observation, even a full history is no help in predicting the next
state.

\section{Atari Games}
The state of an Atari game is fully specified by the 1024-bits of
console RAM. The Atari environment defines an observation function
$O(o|s)$ that maps RAM states to observed screens. This true state is
invisible to humans, who instead see the $160 \times 210 \times 3$
dimensional screen. Fortunately, the Arcade Learning Environment
provides access to both the RAM states and observed Atari screens.

A single Atari game screen often does not contain enough information
to disambiguate important aspects of state. Two examples: in the game
of Breakout, a single frame is sufficient to locate the position, but
not velocity, of the ball, which is necessary in order to correctly
position the paddle. Similarly, the player and opponent bullets in
Space Invaders look identical, so the only way to tell if a bullet is
approaching your ship or leaving it is to look at more than one
screen.

\subsection{Atari RAM Entropy}
We expect the RAM-entropy of will differ on a game-to-game
basis. There are several problems applying the standard definition of
entropy (Def \ref{def:entropy}) to Atari RAM-states:

\subsection{Station State Distribution}
To compute entropy, a distribution over RAM-states is required. In
general, the distribution over visited states is a function of the
policy.

\begin{definition}[Ergodic MDP]
An MDP $\mathcal{M}$ is ergodic if the Markov chain induced by any
policy is ergodic. An ergodic MDP is an MDP where every state can be
accessed in a finite number of steps from any other state.
\end{definition}

\begin{definition}[Episodic MDP]
An episodic MDP is an MDP with a terminal state that is accessible
from any state.
\end{definition}

\begin{definition}[Stationary State Distribution of Ergodic MDP]
An ergodic MDP has a stationary distribution $d^{\pi}(s)$ with the property:
\[
d^\pi(s) = \sum_{s' \in S} d^\pi(s')\mathcal{P}_{s',s}
\]
\end{definition}

Atari games are episodic MDPs with deterministic transitions. Formally
episodic MDPs can never be ergodic since terminal states never
transition to any other state. However, it is possible to convert an
episodic MDP into an ergodic MDP by constructing an equivalent MDP
with each terminal state having a single action that transitions the
agent to the starting state. The transition from the terminal to
starting state should have a discount factor of zero in order to
maintain equivalence of optimal values \cite{VanHasselt11}.

\begin{algorithm}
\caption{Dataset Collection}
\label{ram-ent}
\begin{algorithmic}
\State Given: Policy $\pi$
\State $\mathcal{D} \gets \{\emptyset\}$ \Comment{$\mathcal{D}$ is the collection of all trajectories}
\Do
\State $\mathcal{D} \gets \mathcal{D}\ \cup \ $PlayEpisode$(\pi)$
\State $d^\pi_t \gets \ $ComputeStationaryStateDistribution($\mathcal{D}$)
\State $t\gets t+1$
\doWhile{$D_{KL}(d^\pi_t\ ||\ d^\pi_{t-1}) > \epsilon$} \Comment{Wait until the state distribution converges}
\State \Return $d^\pi_t$
\end{algorithmic}
\end{algorithm}

\begin{definition}[Observability]
Assuming access to the true state distribution $S$ of an environment
as well as the observation distribution $O$, the observability
$\mathcal{O}$ of the environment is given by conditional entropy of
the true states given the observations, divided by the entropy of the
states.
\[
\mathcal{O} = 1 - \frac{H(S|O)}{H(S)}
\]
The observability quantifies how much information the observations
possess about the true environment state. Observability is always
between zero and one: an observability of zero indicates the
observations are no help in understanding the environment's state. An
observability of one implies the observations fully determine the true
state. Observability measure the degree of bijectivity in an
environment's mapping from states to observations.
\end{definition}


\section{Half Baked Ideas}
The following sections contain intuitions that may have some promise
but are poorly grounded.

\section{Sub-optimality induced by partial observability}
Partial observability causes agents to learn sub-optimal policies. The
degree of partial observability inherent in an environment bounds the
performance of an optimal policy learned over observations $\pi_O^*$
compared to the optimal policy learned over true states $\pi_S^*$:
\[
\textrm{sub-optimality} \propto \sum_{s\in S} D_{KL}\big(\pi_O^*(O(s))\ ||\ \pi_S^*(s)\big)
\]

\bibliographystyle{unsrt}
\bibliography{partialobs}
\end{document}
